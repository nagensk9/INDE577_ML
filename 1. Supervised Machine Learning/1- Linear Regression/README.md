# Linear Regression: Concepts, Formulas, and Evaluation

## Overview
- Linear Regression is a simple yet powerful and widely used algorithm in data science.
- This comprehensive guide will introduce you to Linear Regression.

## Introduction
- Models in machine learning learn from data, similar to how humans learn from their experiences.
- Machine learning models can be categorized into supervised learning and unsupervised learning.
- Supervised learning includes regression and classification tasks.
- Regression predicts a continuous output variable, such as scores or prices, while classification predicts categorical outputs like spam or not spam, yes or no, etc.

## Simple Linear Regression
- Linear regression is a statistical regression method used for predictive analysis in machine learning.
- It shows the linear relationship between the independent (predictor) variable (X-axis) and the dependent (output) variable (Y-axis).
- When there is only one input variable, it is called simple linear regression.
- The relationship between the dependent variable (y) and the independent variable (x) is represented by the equation: 

    y = β<sub>0</sub> + β<sub>1</sub>x
    
    where y is the dependent variable, x is the independent variable, β<sub>0</sub> is the constant (intercept), and β<sub>1</sub> is the slope (coefficient).

- The goal of linear regression is to find the best-fit line that minimizes the error between the predicted values and the actual values.
- This is achieved by determining the optimal values for the intercept (β<sub>0</sub>) and slope (β<sub>1</sub>) of the line.
<p align="center"><img src="https://github.com/nagensk9/INDE577_ML/blob/main/Images/Linear_regression_formula.jpg?raw=true" ></p><br>

## Best Fit Line

- The best fit line is a line that fits the given scatter plot in the best way.
- Mathematically, the best fit line is obtained by minimizing the Residual Sum of Squares (RSS), which measures the difference between the expected and actual observed outputs.
- The equation for RSS is:

    RSS = Σ(y<sub>predicted</sub> - y<sub>i</sub>)<sup>2</sup>
    
    where y<sub>predicted</sub> is the predicted value, and y<sub>i</sub> is the actual observed value for each data point.

- The best fit line is obtained by minimizing the RSS and finding the values of β<sub>0</sub> and β<sub>1</sub> that result in the minimum error.
<p align="center"><img src="https://github.com/nagensk9/INDE577_ML/blob/main/Images/Linear_regression.gif?raw=true" width=400></p>

## Cost Function for Linear Regression
- The cost function helps determine the optimal values for β<sub>0</sub> and β<sub>1</sub> that provide the best fit line for the data points.
- In linear regression, the Mean Squared Error (MSE) cost function is commonly used.
- MSE is the average of squared errors between the predicted values and the actual values.
- It is calculated using the simple linear equation y = mx + b:

    MSE = (1/n) * Σ(y - (β<sub>0</sub> + β<sub>1</sub>x))<sup>2</sup>
    
    where n is the number of data points.

- The values of β<sub>0</sub> and β<sub>1</sub> are updated iteratively using an optimization algorithm, such as Gradient Descent, to minimize the MSE and find the optimal solution.

## Gradient Descent for Linear Regression
<p align="center"><img src="https://github.com/nagensk9/INDE577_ML/blob/main/Images/cost_function.gif?raw=true" widt800></p>
- Gradient Descent for Linear Regression is an optimization algorithm used to find the optimal values of β<sub>0</sub> and β<sub>1</sub> by minimizing the cost function (MSE).
- The goal is to iteratively update the values of β<sub>0</sub> and β<sub>1</sub> until reaching the minimum of the cost function.

- The update step in Gradient Descent can be represented by the following equations:

    β<sub>0</sub> = β<sub>0</sub> - α * ∂MSE/∂β<sub>0</sub>
    
    β<sub>1</sub> = β<sub>1</sub> - α * ∂MSE/∂β<sub>1</sub>

    where α is the learning rate, and ∂MSE/∂β<sub>0</sub> and ∂MSE/∂β<sub>1</sub> are the partial derivatives of the MSE with respect to β<sub>0</sub> and β<sub>1</sub> respectively.

- By iteratively updating the values of β<sub>0</sub> and β<sub>1</sub> using the above equations, the algorithm gradually converges to the optimal values that minimize the cost function.


## Evaluation Metrics for Linear Regression
The strength of any linear regression model can be assessed using various evaluation metrics. These evaluation metrics provide a measure of how well the observed outputs are being generated by the model.

### Coefficient of Determination or R-Squared (R2)
<p align="center"><img src="https://github.com/nagensk9/INDE577_ML/blob/main/Images/rsquare.jpg?raw=true" width=500></p>
R-Squared is a commonly used metric that explains the amount of variation that is explained/captured by the developed model. It always ranges between 0 and 1, where a higher value indicates a better fit of the model to the data.

Mathematically, R-Squared can be represented as:

R^2 = 1 - (RSS/TSS)

Where:
- RSS (Residual Sum of Squares) is the sum of squares of the residual for each data point, which measures the difference between the predicted and the actual observed output.
- TSS (Total Sum of Squares) is the sum of errors of the data points from the mean of the response variable.

R-Squared provides an indication of how much of the total variation in the dependent variable can be explained by the independent variable(s). A value of 1 indicates that the model explains all the variation, while a value of 0 indicates that the model does not explain any variation beyond the mean of the dependent variable.
<p align="center"><img src="https://github.com/nagensk9/INDE577_ML/blob/main/Images/rsquare_comp.jpg?raw=true"></p>
### Root Mean Squared Error (RMSE)
The Root Mean Squared Error is another commonly used metric to evaluate the performance of a linear regression model. It calculates the square root of the variance of the residuals, providing a measure of the absolute fit of the model to the data.

Mathematically, RMSE can be represented as:

RMSE = sqrt((1/n) * Σ(y_predicted - y_actual)²)

Where:
- n: Number of data points
- y_predicted: Predicted value
- y_actual: Actual value
### Residual Standard Error (RSE)
- RSE is similar to RMSE but is calculated by dividing the sum of squared residuals by the degrees of freedom rather than the total number of data points.
- It is a measure of the standard deviation of the residuals.
- RSE is calculated as:

RSE = sqrt((1/(n-2)) * Σ(y_predicted - y_actual)²)

Where:
- n: Number of data points
- y_predicted: Predicted value
- y_actual: Actual value
RMSE quantifies the average magnitude of the residuals and indicates how close the observed data points are to the predicted values. A lower RMSE value indicates a better fit of the model to the data.
